{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: amazon_products.csv\n",
      "Skipping 10001: Table not found\n",
      "Skipping 10002: Table not found\n",
      "Skipping 10003: Table not found\n",
      "Skipping 10004: Table not found\n",
      "Skipping 10022: Table not found\n",
      "Skipping 10024: Table not found\n",
      "Skipping 10029: Table not found\n",
      "Skipping 10035: Table not found\n",
      "Skipping 10051: Table not found\n",
      "Skipping 10055: Table not found\n",
      "Skipping 10059: Table not found\n",
      "Saved: amazon_mobile.csv\n",
      "Skipping 10064: Table not found\n",
      "Skipping 10069: Table not found\n",
      "Skipping 10083: Table not found\n",
      "Skipping 10085: Table not found\n",
      "Skipping 10089: Table not found\n",
      "Skipping 10090: Table not found\n",
      "Skipping 10100: Table not found\n",
      "Saved: amazon_ipad.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set headers to mimic a browser request\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "BASE_URL = 'https://www.amazon.sg'\n",
    "PRODUCT_DICT = {\n",
    "    'laptop': \"https://www.amazon.sg/s?k=laptop\",\n",
    "    #'mobile': \"https://www.amazon.sg/s?k=mobile\",\n",
    "    #'ipad': \"https://www.amazon.sg/s?k=ipad\",\n",
    "    'grocery':\"https://www.amazon.sg/s?k=grocery\"\n",
    "}\n",
    "\n",
    "def scrape_product_list():\n",
    "    \"\"\"Scrape the product listing page for product details.\"\"\"\n",
    "    product_data = []\n",
    "    prod_id = 10000  # Starting ID for products\n",
    "\n",
    "    for category, url in PRODUCT_DICT.items():\n",
    "        response = requests.get(url, headers=HEADERS)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find all product containers\n",
    "        products = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
    "\n",
    "        # Extract product details\n",
    "        for product in products:\n",
    "            title_element = product.find('h2', {'aria-label': True})\n",
    "            title = title_element['aria-label'] if title_element else \"N/A\"\n",
    "\n",
    "            rating_element = product.find('i', {'data-cy': 'reviews-ratings-slot'})\n",
    "            rating = rating_element.find('span', {'class': 'a-icon-alt'}).text if rating_element else \"N/A\"\n",
    "\n",
    "            review_div = product.find('div', class_='a-row a-size-small')\n",
    "            review_tag = review_div.find('a', class_='a-link-normal s-underline-text s-underline-link-text s-link-style') if review_div else None\n",
    "            review_count = review_tag.find('span').text.strip() if review_tag else \"Not found\"\n",
    "\n",
    "            price_symbol = product.find('span', class_='a-price-symbol')\n",
    "            price_whole = product.find('span', class_='a-price-whole')\n",
    "            price_fraction = product.find('span', class_='a-price-fraction')\n",
    "\n",
    "            price = (price_symbol.text.strip() if price_symbol else \"\") + \\\n",
    "                    (price_whole.text.strip() if price_whole else \"\") + \\\n",
    "                    (price_fraction.text.strip() if price_fraction else \"\")\n",
    "\n",
    "            delivery_div = product.find('div', {'data-cy': 'delivery-recipe'})\n",
    "            delivery_date_span = delivery_div.find('span', {'aria-label': True}) if delivery_div else None\n",
    "            delivery = delivery_date_span['aria-label'] if delivery_date_span else \"Not found\"\n",
    "\n",
    "            link_tag = product.find('a', class_='a-link-normal')\n",
    "            product_link = link_tag['href'] if link_tag else \"Not found\"\n",
    "            full_link = BASE_URL + product_link if product_link.startswith(\"/\") else product_link\n",
    "\n",
    "            prod_id += 1\n",
    "            product_data.append([prod_id, category, title, rating, review_count, price, delivery, full_link])\n",
    "\n",
    "    return pd.DataFrame(product_data, columns=['prod_id', 'prod_type', 'title', 'rating', 'review_count', 'price', 'delivery', 'link'])\n",
    "\n",
    "\n",
    "def scrape_product_details(df):\n",
    "    \"\"\"Scrape additional product details for each category.\"\"\"\n",
    "    for category in PRODUCT_DICT.keys():\n",
    "        product_details_list = []\n",
    "\n",
    "        for _, record in df[df['prod_type'] == category].iterrows():\n",
    "            response = requests.get(record['link'], headers=HEADERS)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            parent_div = soup.find('div', class_=['a-section a-spacing-small a-spacing-top-small'])\n",
    "            if not parent_div:\n",
    "                print(f\"Skipping {record['prod_id']}: Table not found\")\n",
    "                continue\n",
    "\n",
    "            table = parent_div.find('table', class_='a-normal a-spacing-micro')\n",
    "            if not table:\n",
    "                print(f\"Skipping {record['prod_id']}: Table not found\")\n",
    "                continue\n",
    "\n",
    "            rows = table.find_all('tr')\n",
    "            product_details = {'prod_id': record['prod_id']}\n",
    "\n",
    "            for row in rows:\n",
    "                key = row.find('td', class_='a-span3').get_text(strip=True)\n",
    "                value = row.find('td', class_='a-span9').get_text(strip=True)\n",
    "                product_details[key] = value\n",
    "\n",
    "            product_details_list.append(product_details)\n",
    "\n",
    "        df_details = pd.DataFrame(product_details_list)\n",
    "        save_to_csv(df_details, f'amazon_{category}.csv')\n",
    "\n",
    "\n",
    "def save_to_csv(df, filename):\n",
    "    \"\"\"Save DataFrame to CSV.\"\"\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Saved: {filename}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute the workflow.\"\"\"\n",
    "    df = scrape_product_list()\n",
    "    save_to_csv(df, 'amazon_products.csv')\n",
    "    \n",
    "    scrape_product_details(df)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bde",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
